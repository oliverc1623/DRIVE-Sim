{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b21828c-fb99-4d4b-934c-a5f2216cfdce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../vista_nautilus/models/')\n",
    "from transformer import Transformer\n",
    "\n",
    "class ActorCriticModel(nn.Module):\n",
    "    def __init__(self, config, observation_space, action_space_shape, max_episode_length):\n",
    "        \"\"\"Model setup\n",
    "\n",
    "        Arguments:\n",
    "            config {dict} -- Configuration and hyperparameters of the environment, trainer and model.\n",
    "            observation_space {box} -- Properties of the agent's observation space\n",
    "            action_space_shape {tuple} -- Dimensions of the action space\n",
    "            max_episode_length {int} -- The maximum number of steps in an episode\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_layer_size\"]\n",
    "        self.memory_layer_size = config[\"transformer\"][\"embed_dim\"]\n",
    "        self.observation_space_shape = observation_space.shape\n",
    "        self.max_episode_length = max_episode_length\n",
    "\n",
    "        # Observation encoder\n",
    "        if len(self.observation_space_shape) > 1:\n",
    "            # Case: visual observation is available\n",
    "            # Visual encoder made of 3 convolutional layers\n",
    "            self.conv1 = nn.Conv2d(observation_space.shape[0], 32, 8, 4,)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 4, 2, 0)\n",
    "            self.conv3 = nn.Conv2d(64, 64, 3, 1, 0)\n",
    "            nn.init.orthogonal_(self.conv1.weight, np.sqrt(2))\n",
    "            nn.init.orthogonal_(self.conv2.weight, np.sqrt(2))\n",
    "            nn.init.orthogonal_(self.conv3.weight, np.sqrt(2))\n",
    "            # Compute output size of convolutional layers\n",
    "            self.conv_out_size = self.get_conv_output(observation_space.shape)\n",
    "            in_features_next_layer = self.conv_out_size\n",
    "        else:\n",
    "            # Case: vector observation is available\n",
    "            in_features_next_layer = observation_space.shape[0]\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.lin_hidden = nn.Linear(in_features_next_layer, self.memory_layer_size)\n",
    "        nn.init.orthogonal_(self.lin_hidden.weight, np.sqrt(2))\n",
    "\n",
    "        # Transformer Blocks\n",
    "        self.transformer = Transformer(config[\"transformer\"], self.memory_layer_size, self.max_episode_length)\n",
    "\n",
    "        # Decouple policy from value\n",
    "        # Hidden layer of the policy\n",
    "        self.lin_policy = nn.Linear(self.memory_layer_size, 2)\n",
    "        nn.init.orthogonal_(self.lin_policy.weight, np.sqrt(2))\n",
    "\n",
    "        # Hidden layer of the value function\n",
    "        self.lin_value = nn.Linear(self.memory_layer_size, self.hidden_size)\n",
    "        nn.init.orthogonal_(self.lin_value.weight, np.sqrt(2))\n",
    "\n",
    "        # Outputs / Model heads\n",
    "        # Policy (Coninutous Normal distribution)\n",
    "        self.policy_branches = nn.ModuleList()\n",
    "        # for num_actions in action_space_shape:\n",
    "        #     actor_branch = nn.Linear(in_features=self.hidden_size, out_features=num_actions)\n",
    "        #     nn.init.orthogonal_(actor_branch.weight, np.sqrt(0.01))\n",
    "        #     self.policy_branches.append(actor_branch)\n",
    "            \n",
    "        # Value function\n",
    "        self.value = nn.Linear(self.hidden_size, 1)\n",
    "        nn.init.orthogonal_(self.value.weight, 1)\n",
    "\n",
    "    def forward(self, obs:torch.tensor, memory:torch.tensor, memory_mask:torch.tensor, memory_indices:torch.tensor):\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Arguments:\n",
    "            obs {torch.tensor} -- Batch of observations\n",
    "            memory {torch.tensor} -- Episodic memory window\n",
    "            memory_mask {torch.tensor} -- Mask to prevent the model from attending to the padding\n",
    "            memory_indices {torch.tensor} -- Indices to select the positional encoding that matches the memory window\n",
    "\n",
    "        Returns:\n",
    "            {Categorical} -- Policy: Categorical distribution\n",
    "            {torch.tensor} -- Value function: Value\n",
    "        \"\"\"\n",
    "        # Set observation as input to the model\n",
    "        h = obs\n",
    "        # Forward observation encoder\n",
    "        if len(self.observation_space_shape) > 1:\n",
    "            batch_size = h.size()[0]\n",
    "            # Propagate input through the visual encoder\n",
    "            h = F.relu(self.conv1(h))\n",
    "            h = F.relu(self.conv2(h))\n",
    "            h = F.relu(self.conv3(h))\n",
    "            # Flatten the output of the convolutional layers\n",
    "            h = h.reshape((batch_size, -1))\n",
    "\n",
    "        # Feed hidden layer\n",
    "        h = F.relu(self.lin_hidden(h))\n",
    "        \n",
    "        # Forward transformer blocks\n",
    "        h, memory = self.transformer(h, memory, memory_mask, memory_indices)\n",
    "\n",
    "        # Decouple policy from value\n",
    "        # Feed hidden layer (policy)\n",
    "        h_policy = F.relu(self.lin_policy(h))\n",
    "        # Feed hidden layer (value function)\n",
    "        h_value = F.relu(self.lin_value(h))\n",
    "        # Head: Value function\n",
    "        value = self.value(h_value).reshape(-1)\n",
    "        # Head: Policy\n",
    "        # pi = [Normal(logits=branch(h_policy)) for branch in self.policy_branches]\n",
    "        mu, log_sigma = torch.chunk(h_policy, 2, dim=-1)\n",
    "        mu = 1/8.0 * torch.tanh(mu)  # conversion\n",
    "        sigma = 0.1 * torch.sigmoid(log_sigma) + 0.005  # conversion\n",
    "        pi = Normal(mu, sigma)\n",
    "\n",
    "        return pi, value, memory\n",
    "\n",
    "    def get_conv_output(self, shape:tuple) -> int:\n",
    "        \"\"\"Computes the output size of the convolutional layers by feeding a dummy tensor.\n",
    "\n",
    "        Arguments:\n",
    "            shape {tuple} -- Input shape of the data feeding the first convolutional layer\n",
    "\n",
    "        Returns:\n",
    "            {int} -- Number of output features returned by the utilized convolutional layers\n",
    "        \"\"\"\n",
    "        o = self.conv1(torch.zeros(1, *shape))\n",
    "        o = self.conv2(o)\n",
    "        o = self.conv3(o)\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def get_grad_norm(self):\n",
    "        \"\"\"Returns the norm of the gradients of the model.\n",
    "        \n",
    "        Returns:\n",
    "            {dict} -- Dictionary of gradient norms grouped by layer name\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        if len(self.observation_space_shape) > 1:\n",
    "            grads[\"encoder\"] = self._calc_grad_norm(self.conv1, self.conv2, self.conv3)  \n",
    "            \n",
    "        grads[\"linear_layer\"] = self._calc_grad_norm(self.lin_hidden)\n",
    "        \n",
    "        transfomer_blocks = self.transformer.transformer_blocks\n",
    "        for i, block in enumerate(transfomer_blocks):\n",
    "            grads[\"transformer_block_\" + str(i)] = self._calc_grad_norm(block)\n",
    "        \n",
    "        for i, head in enumerate(self.policy_branches):\n",
    "            grads[\"policy_head_\" + str(i)] = self._calc_grad_norm(head)\n",
    "        \n",
    "        grads[\"lin_policy\"] = self._calc_grad_norm(self.lin_policy)\n",
    "        grads[\"value\"] = self._calc_grad_norm(self.lin_value, self.value)\n",
    "        grads[\"model\"] = self._calc_grad_norm(self, self.value)\n",
    "          \n",
    "        return grads\n",
    "    \n",
    "    def _calc_grad_norm(self, *modules):\n",
    "        \"\"\"Computes the norm of the gradients of the given modules.\n",
    "\n",
    "        Arguments:\n",
    "            modules {list} -- List of modules to compute the norm of the gradients of.\n",
    "\n",
    "        Returns:\n",
    "            {float} -- Norm of the gradients of the given modules. \n",
    "        \"\"\"\n",
    "        grads = []\n",
    "        for module in modules:\n",
    "            for name, parameter in module.named_parameters():\n",
    "                grads.append(parameter.grad.view(-1))\n",
    "        return torch.linalg.norm(torch.cat(grads)).item() if len(grads) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e824a520-af0e-4bc5-8482-bf34c7fac53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\"hidden_layer_size\": 128,\n",
    "          \"transformer\": \n",
    "               {\"num_blocks\": 4,\n",
    "                \"embed_dim\": 128,\n",
    "                \"num_heads\": 1,\n",
    "                \"memory_length\": 32,\n",
    "                \"positional_encoding\": \"\", # options: \"\" \"relative\" \"learned\"\n",
    "                \"layer_norm\": \"pre\" ,# options: \"\" \"pre\" \"post\"\n",
    "                \"gtrxl\": True,\n",
    "                \"gtrxl_bias\": 0.0}}\n",
    "observation_space = torch.rand((3,70,310))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9f76961-b981-4c29-aefa-c7eb9381d5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticModel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (lin_hidden): Linear(in_features=11200, out_features=128, bias=True)\n",
       "  (transformer): Transformer(\n",
       "    (activation): ReLU()\n",
       "    (linear_embedding): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-3): 4 x TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (values): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (keys): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (queries): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (gate1): GRUGate(\n",
       "          (Wr): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Ur): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Wz): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Uz): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Wg): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Ug): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (tanh): Tanh()\n",
       "        )\n",
       "        (gate2): GRUGate(\n",
       "          (Wr): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Ur): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Wz): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Uz): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Wg): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (Ug): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (tanh): Tanh()\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_kv): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lin_policy): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (lin_value): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (policy_branches): ModuleList()\n",
       "  (value): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ActorCriticModel(config, observation_space, [2], 500)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b3c5ff7-0f71-4f2a-8576-8a030a924089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,  ...,   29,   30,   31],\n",
      "        [   0,    1,    2,  ...,   29,   30,   31],\n",
      "        [   0,    1,    2,  ...,   29,   30,   31],\n",
      "        ...,\n",
      "        [2966, 2967, 2968,  ..., 2995, 2996, 2997],\n",
      "        [2967, 2968, 2969,  ..., 2996, 2997, 2998],\n",
      "        [2968, 2969, 2970,  ..., 2997, 2998, 2999]])\n"
     ]
    }
   ],
   "source": [
    "memory_length = config[\"transformer\"][\"memory_length\"]\n",
    "num_blocks = config[\"transformer\"][\"num_blocks\"]\n",
    "embed_dim = config[\"transformer\"][\"embed_dim\"]\n",
    "max_episode_length = 3000\n",
    "# Setup placeholders for each worker's current episodic memory\n",
    "memory = torch.zeros((4, 3000, num_blocks, embed_dim), dtype=torch.float32)\n",
    "# Generate episodic memory mask used in attention\n",
    "memory_mask = torch.tril(torch.ones((memory_length, memory_length)), diagonal=-1)\n",
    "\"\"\" e.g. memory mask tensor looks like this if memory_length = 6\n",
    "0, 0, 0, 0, 0, 0\n",
    "1, 0, 0, 0, 0, 0\n",
    "1, 1, 0, 0, 0, 0\n",
    "1, 1, 1, 0, 0, 0\n",
    "1, 1, 1, 1, 0, 0\n",
    "1, 1, 1, 1, 1, 0\n",
    "\"\"\"         \n",
    "# Setup memory window indices to support a sliding window over the episodic memory\n",
    "repetitions = torch.repeat_interleave(torch.arange(0, memory_length).unsqueeze(0), memory_length - 1, dim = 0).long()\n",
    "memory_indices = torch.stack([torch.arange(i, i + memory_length) for i in range(max_episode_length - memory_length + 1)]).long()\n",
    "memory_indices = torch.cat((repetitions, memory_indices))\n",
    "\"\"\" e.g. the memory window indices tensor looks like this if memory_length = 4 and max_episode_length = 7:\n",
    "0, 1, 2, 3\n",
    "0, 1, 2, 3\n",
    "0, 1, 2, 3\n",
    "0, 1, 2, 3\n",
    "1, 2, 3, 4\n",
    "2, 3, 4, 5\n",
    "3, 4, 5, 6\n",
    "\"\"\"\n",
    "print(memory_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1115eed6-5d59-4888-8e7c-ecb981580db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 3000, 1, 128]' is invalid for input of size 1536000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m70\u001b[39m, \u001b[38;5;241m310\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 102\u001b[0m, in \u001b[0;36mActorCriticModel.forward\u001b[0;34m(self, obs, memory, memory_mask, memory_indices)\u001b[0m\n\u001b[1;32m     99\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_hidden(h))\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Forward transformer blocks\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m h, memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Decouple policy from value\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Feed hidden layer (policy)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m h_policy \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_policy(h))\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ExplainableRL/notebooks/../vista_nautilus/models/transformer.py:249\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, h, memories, mask, memory_indices)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks):\n\u001b[1;32m    248\u001b[0m     out_memories\u001b[38;5;241m.\u001b[39mappend(h\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m--> 249\u001b[0m     h, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemories\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemories\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# args: value, key, query, mask\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(h\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ExplainableRL/notebooks/../vista_nautilus/models/transformer.py:137\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, value, key, query, mask)\u001b[0m\n\u001b[1;32m    134\u001b[0m     query_ \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Forward MultiHeadAttention\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m attention, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# GRU Gate or skip connection\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gtrxl:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Forward GRU gating\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ExplainableRL/notebooks/../vista_nautilus/models/transformer.py:50\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, values, keys, query, mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m value_len, key_len, query_len \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], keys\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], query\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Split the embedding into self.num_heads different pieces\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m keys \u001b[38;5;241m=\u001b[39m keys\u001b[38;5;241m.\u001b[39mreshape(N, key_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size)\n\u001b[1;32m     52\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mreshape(N, query_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 3000, 1, 128]' is invalid for input of size 1536000"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1,3,70, 310))\n",
    "model(x, memory, memory_mask, memory_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vista kernel",
   "language": "python",
   "name": "vista"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
