{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c47b945-81d3-49c1-b0d2-ef75fc5bde87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56750155-589f-4cf7-8be4-786200f1707d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3d8b41-e26a-4dcc-a193-d13fe30ed2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_encoder_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.linear_mu = nn.Linear(input_size, 1)\n",
    "        self.linear_sigma = nn.Linear(input_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        mu = self.linear_mu(x.mean(dim=0))  # Global average pooling across the time dimension\n",
    "        sigma = torch.exp(self.linear_sigma(x.mean(dim=0)))  # Exponentiate to ensure positive values\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653240ca-d6be-43ee-85d3-908278dfda21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfDrivingCarModel(nn.Module):\n",
    "    def __init__(self, cnn, transformer_encoder):\n",
    "        super(SelfDrivingCarModel, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        # Update the input_size to match the output size of the CNN\n",
    "        self.transformer_encoder = transformer_encoder  # Output size of the CNN\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cnn_output = self.cnn(x)\n",
    "        cnn_output = cnn_output.flatten(start_dim=2).permute(2, 0, 1)  # Reshape for transformer input\n",
    "        mu, sigma = self.transformer_encoder(cnn_output)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "498d3200-5d38-4786-942a-5ebaaa22a364",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5x32 and 2048x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m input_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Pass the input images through the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m mu, sigma \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMu (mean) predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(mu)\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m, in \u001b[0;36mSelfDrivingCarModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m cnn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)\n\u001b[1;32m     10\u001b[0m cnn_output \u001b[38;5;241m=\u001b[39m cnn_output\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape for transformer input\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m mu, sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mu, sigma\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)\n\u001b[0;32m---> 13\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_mu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Global average pooling across the time dimension\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_sigma(x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))  \u001b[38;5;66;03m# Exponentiate to ensure positive values\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mu, sigma\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x32 and 2048x1)"
     ]
    }
   ],
   "source": [
    "# Create the CNN and Transformer Encoder\n",
    "cnn = CNN()\n",
    "transformer_encoder = TransformerEncoder(input_size=32 * 8 * 8, d_model=32, nhead=4, num_encoder_layers=2)\n",
    "\n",
    "# Create the complete model\n",
    "model = SelfDrivingCarModel(cnn, transformer_encoder)\n",
    "\n",
    "# Generate random input images (batch_size=2, 3 channels, 32x32 resolution)\n",
    "input_images = torch.randn(2, 3, 32, 32)\n",
    "\n",
    "# Pass the input images through the model\n",
    "mu, sigma = model(input_images)\n",
    "\n",
    "print(\"Mu (mean) predictions:\")\n",
    "print(mu)\n",
    "\n",
    "print(\"\\nSigma (standard deviation) predictions:\")\n",
    "print(sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f87522a4-33ac-4d10-ab34-d14411b2f7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np \n",
    "import math \n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Standard 2 layerd FFN of transformer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.3):\n",
    "        super(FeedForward, self).__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        nn.init.normal(self.linear_1.weight, std=0.001)  \n",
    "        nn.init.normal(self.linear_2.weight, std=0.001)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "# standard NORM layer of Transformer\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6, trainable=True):\n",
    "        super(Norm, self).__init__()\n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        if trainable:\n",
    "            self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "            self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        else:\n",
    "            self.alpha = nn.Parameter(torch.ones(self.size), requires_grad=False)\n",
    "            self.bias = nn.Parameter(torch.zeros(self.size), requires_grad=False)\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "\n",
    "# Standard positional encoding (addition/ concat both are valid) \n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        batch_size = x.size(0)\n",
    "        num_feature = x.size(2)\n",
    "        spatial_h = x.size(3)\n",
    "        spatial_w = x.size(4)\n",
    "        z = Variable(self.pe[:,:seq_len],requires_grad=False)\n",
    "        z = z.unsqueeze(-1).unsqueeze(-1)\n",
    "        z = z.expand(batch_size,seq_len, num_feature, spatial_h,  spatial_w)\n",
    "        x = x + z\n",
    "        return x\n",
    "\n",
    "\n",
    "# standard attention layer\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.sum(q * k , -1)/  math.sqrt(d_k)\n",
    "    # scores : b, t \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    scores = scores.unsqueeze(-1).expand(scores.size(0), scores.size(1), v.size(-1))\n",
    "    # scores : b, t, dim \n",
    "    output = scores * v\n",
    "    output = torch.sum(output,1)\n",
    "    if dropout:\n",
    "        output = dropout(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TX(nn.Module):\n",
    "    def __init__(self, d_model=64 , dropout = 0.3 ):\n",
    "        super(TX, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        # no of head has been modified to encompass : 1024 dimension \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff=d_model/2)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q: (b , dim )\n",
    "        b = q.size(0)\n",
    "        t = k.size(1)\n",
    "        dim = q.size(1)\n",
    "        q_temp = q.unsqueeze(1)\n",
    "        q_temp= q_temp.expand(b, t , dim)\n",
    "        # q,k,v : (b, t , d_model=1024 // 16 )\n",
    "        A = attention(q_temp, k, v, self.d_model, mask, self.dropout)\n",
    "        # A : (b , d_model=1024 // 16 )\n",
    "        q_ = self.norm_1(A + q)\n",
    "        new_query = self.norm_2(q_ +  self.dropout_2(self.ff(q_))) \n",
    "        return new_query\n",
    "\n",
    "\n",
    "class Block_head(nn.Module):\n",
    "    def __init__(self, d_model=64 , dropout = 0.3 ):\n",
    "        super(Block_head, self).__init__()\n",
    "        self.T1 = TX()\n",
    "        self.T2 = TX()\n",
    "        self.T3 = TX()\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q = self.T1(q,k,v)\n",
    "        q = self.T2(q,k,v)\n",
    "        q = self.T3(q,k,v)\n",
    "        return q\n",
    "\n",
    "\n",
    "class Tail(nn.Module):\n",
    "    def __init__(self, num_classes , num_frames, head=16):\n",
    "        super(Tail, self).__init__()\n",
    "        self.spatial_h = 7\n",
    "        self.spatial_w = 4\n",
    "        self.head = head\n",
    "        self.num_features = 2048\n",
    "        self.num_frames = num_frames \n",
    "        self.d_model = self.num_features / 2\n",
    "        self.d_k = self.d_model // self.head\n",
    "        self.bn1 = nn.BatchNorm2d(self.num_features)\n",
    "        self.bn2 = Norm(self.d_model, trainable=False)\n",
    "        \n",
    "        self.pos_embd = PositionalEncoder(self.num_features, self.num_frames)\n",
    "        self.Qpr = nn.Conv2d(self.num_features, self.d_model, kernel_size=(7,4), stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.head_layers =[]\n",
    "        for i in range(self.head):\n",
    "            self.head_layers.append(Block_head())\n",
    "\n",
    "        self.list_layers = nn.ModuleList(self.head_layers)\n",
    "        self.classifier = nn.Linear(self.d_model, num_classes)\n",
    "        # resnet style initialization \n",
    "        nn.init.kaiming_normal(self.Qpr.weight, mode='fan_out')\n",
    "        nn.init.normal(self.classifier.weight, std=0.001)  \n",
    "        # nn.init.constant(self.classifier.bias, 0)\n",
    "        \n",
    "        nn.init.constant(self.bn1.weight , 1)\n",
    "        nn.init.constant(self.bn1.bias , 0)\n",
    "        \n",
    "    def forward(self, x, b , t ):\n",
    "        x = self.bn1(x)\n",
    "        # stabilizes the learning\n",
    "        x = x.view(b , t , self.num_features , self.spatial_h , self.spatial_w)\n",
    "        x = self.pos_embd(x)\n",
    "        x = x.view(-1, self.num_features , self.spatial_h , self.spatial_w)\n",
    "        x = F.relu(self.Qpr(x))\n",
    "        # x: (b,t,1024,1,1) since its a convolution: spatial positional encoding is not added \n",
    "        # paper has a different base (resnet in this case): which 2048 x 7 x 4 vs 16 x 7 x 7 \n",
    "        x = x.view(-1, t ,  self.d_model )\n",
    "        x = self.bn2(x)\n",
    "        # stabilization\n",
    "        q = x[:,t/2,:] #middle frame is the query\n",
    "        v = x # value\n",
    "        k = x #key \n",
    "\n",
    "        q = q.view(b, self.head, self.d_k  )\n",
    "        k = k.view(b,t, self.head, self.d_k )\n",
    "        v = v.view(b,t, self.head, self.d_k )\n",
    "\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        #  q: b, 16, 64\n",
    "        #  k,v: b, 16, 10 ,64\n",
    "        outputs = []\n",
    "        for i in range(self.head):\n",
    "            outputs.append(self.list_layers[i](q[:,i],k[:,i], v[:,i]) )\n",
    "\n",
    "        f = torch.cat(outputs, 1)\n",
    "        f = F.normalize(f, p=2, dim=1)\n",
    "        # F.norma\n",
    "        if not self.training:\n",
    "            return f\n",
    "        y = self.classifier(f)\n",
    "        return y, f\n",
    "\n",
    "\n",
    "# base is resnet\n",
    "# Tail is the main transormer network \n",
    "class Semi_Transformer(nn.Module):\n",
    "    def __init__(self, num_classes, seq_len):\n",
    "        super(Semi_Transformer, self).__init__()\n",
    "        resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        self.base = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        self.tail = Tail(num_classes, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        t = x.size(1)\n",
    "        x = x.view(b*t, x.size(2), x.size(3), x.size(4))\n",
    "        x = self.base(x)\n",
    "        # x: (b,t,2048,7,4)\n",
    "        return self.tail(x, b , t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "877e9ba1-a7ce-4873-af8b-f503a8bb05f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/vista/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/vista/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ones(): argument 'size' (position 1) must be tuple of ints, but found element of type float at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSemi_Transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\n",
      "Cell \u001b[0;32mIn[17], line 204\u001b[0m, in \u001b[0;36mSemi_Transformer.__init__\u001b[0;34m(self, num_classes, seq_len)\u001b[0m\n\u001b[1;32m    202\u001b[0m resnet50 \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mresnet50(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(resnet50\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtail \u001b[38;5;241m=\u001b[39m \u001b[43mTail\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 141\u001b[0m, in \u001b[0;36mTail.__init__\u001b[0;34m(self, num_classes, num_frames, head)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2 \u001b[38;5;241m=\u001b[39m \u001b[43mNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embd \u001b[38;5;241m=\u001b[39m PositionalEncoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_frames)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQpr \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m4\u001b[39m), stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m, in \u001b[0;36mNorm.__init__\u001b[0;34m(self, d_model, eps, trainable)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m=\u001b[39m eps\n",
      "\u001b[0;31mTypeError\u001b[0m: ones(): argument 'size' (position 1) must be tuple of ints, but found element of type float at pos 0"
     ]
    }
   ],
   "source": [
    "model = Semi_Transformer(num_classes=2, seq_len = 10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd99f878-dd54-4b01-a1ca-2f4e8cb225ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            d_model=128,\n",
    "            nhead=8,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.transformer(x.unsqueeze(0)).squeeze(0)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbdcc7c8-f801-424f-8136-116f890242e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoder_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menable_nested_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmask_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "TransformerEncoder is a stack of N encoder layers. Users can build the\n",
       "BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.\n",
       "\n",
       "Args:\n",
       "    encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
       "    num_layers: the number of sub-encoder-layers in the encoder (required).\n",
       "    norm: the layer normalization component (optional).\n",
       "    enable_nested_tensor: if True, input will automatically convert to nested tensor\n",
       "        (and convert back on output). This will improve the overall performance of\n",
       "        TransformerEncoder when padding rate is high. Default: ``True`` (enabled).\n",
       "\n",
       "Examples::\n",
       "    >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
       "    >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
       "    >>> src = torch.rand(10, 32, 512)\n",
       "    >>> out = transformer_encoder(src)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/envs/vista/lib/python3.8/site-packages/torch/nn/modules/transformer.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26817765-334f-4e8a-8229-969b7f0a2d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'd_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCNNTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m model(image)\n",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m, in \u001b[0;36mCNNTransformer.__init__\u001b[0;34m(self, num_channels, num_classes)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv2d(num_channels, \u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     11\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     nn\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformerEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m128\u001b[39m, num_classes)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'd_model'"
     ]
    }
   ],
   "source": [
    "image = torch.randn(1, 3, 224, 224)\n",
    "model = CNNTransformer(3, 2)\n",
    "output = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84898e06-ad8d-49f4-a0e5-a41099444cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base kernel",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
